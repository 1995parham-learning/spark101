/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package donbal;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

import java.util.Arrays;
import java.util.Iterator;
import java.util.concurrent.TimeoutException;
import java.util.function.Function;

import org.apache.spark.api.java.function.FlatMapFunction;
import static org.apache.spark.sql.functions.from_json;
import static org.apache.spark.sql.functions.col;
import static org.apache.spark.sql.functions.window;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;

public class App {
  public static void main(String[] args) {
    SparkSession spark = SparkSession
        .builder()
        .master("local")
        .appName("Donbal")
        .getOrCreate();

    // create DataFrame representing the stream of input lines from connection to
    // localhost:1378.
    // use server.py to have server on 1378 that sends messages.
    // please note that columns by default named as value.
    Dataset<Message> messages = spark
        .readStream()
        .format("socket")
        .option("host", "localhost")
        .option("port", 1378)
        .load()
        .withColumn("message", from_json(col("value"), Encoders.bean(Message.class).schema()))
        .select("message.*").as(Encoders.bean(Message.class));

    // split the lines into words with space
    Dataset<Message> words = messages
        .flatMap(new FlatMapFunction<Message, Message>() {
          @Override
          public Iterator<Message> call(Message t) throws Exception {
            return Arrays.asList(t.getLine().split(" ")).stream().map(new Function<String, Message>() {
              @Override
              public Message apply(String word) {
                return new Message(word, t.getTimestamp());
              }
            }).iterator();
          }
        }, Encoders.bean(Message.class));

    // groups words if they are the same and then count them.
    Dataset<WordCount> wordCounts = words.groupBy(
        window(col("timestamp"), "10 minutes", "5 minutes"), col("line").as("value"))
        .count()
        .as(Encoders.bean(WordCount.class));

    // start running the query that prints the running counts to the console
    try {
      StreamingQuery query = wordCounts.writeStream()
          .outputMode("complete")
          .format("console")
          .start();
      query.awaitTermination();

    } catch (StreamingQueryException | TimeoutException exception) {
      exception.printStackTrace();
    }

  }
}
